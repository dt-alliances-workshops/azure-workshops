# Lab 5: AI Observability

## 5.7 Summary

Congratulations! You have completed Lab 5 and learned how to use Dynatrace AI Observability to monitor LLM-powered applications.

In this section, you should have completed the following:

✅ Explored the AI Observability app and located AI-powered services

✅ Monitored LLM service and model health metrics

✅ Analyzed token consumption and cost implications

✅ Troubleshot latency issues in LLM responses

✅ Explored distributed traces to view prompts and responses

✅ Used DQL to query AI observability data in Notebooks

### What's Next?

In **Lab 6**, you will find cleanup instructions for the workshop resources and additional resources for continuing your Dynatrace and Azure journey.
